%pip install streamlit

import pandas as pd
import numpy as np

column_names = [
    'age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',
    'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'
]

url = "https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data"

df = pd.read_csv(url, header=None, names=column_names, na_values='?')

print("Dataset Head:")
print(df.head())
print("\nDataset Info:")
df.info()


import logging
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from typing import List, Dict, Union

# --- Configuration ---
# Central place to manage constants and parameters for the script
CONFIG: Dict[str, Union[str, List[str]]] = {
    "data_url": "https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data",
    "column_names": [
        'age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',
        'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'
    ],
    "cols_to_impute": ["ca", "thal"],
    "categorical_cols": ["sex", "cp", "fbs", "restecg", "exang", "slope", "ca", "thal"],
    "numerical_cols_to_scale": ['age', 'trestbps', 'chol', 'thalach', 'oldpeak'],
    "output_path_prefix": "heart_disease_eda" # Prefix for saved plots
}

# --- Logging Setup ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def load_data(url: str, column_names: List[str]) -> pd.DataFrame:

    logging.info(f"Loading data from {url}...")
    try:
        df = pd.read_csv(url, header=None, names=column_names, na_values='?')
        logging.info("Data loaded successfully.")
        return df
    except Exception as e:
        logging.error(f"Failed to load data: {e}")
        raise

def handle_missing_values(df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:

    logging.info("Handling missing values using median imputation...")
    for col in columns:
        if df[col].isnull().any():
            median_val = df[col].median()
            df[col].fillna(median_val, inplace=True)
            logging.info(f"Imputed missing values in column '{col}' with median value {median_val}.")
    return df

def encode_categorical_features(df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:

    logging.info(f"Applying one-hot encoding to columns: {columns}...")
    df_encoded = pd.get_dummies(df, columns=columns, drop_first=True)
    logging.info(f"DataFrame shape after encoding: {df_encoded.shape}")
    return df_encoded

def scale_numerical_features(df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:

    logging.info(f"Scaling numerical features: {columns}...")
    scaler = StandardScaler()
    df[columns] = scaler.fit_transform(df[columns])
    return df

def perform_exploratory_data_analysis(df: pd.DataFrame, original_df: pd.DataFrame, prefix: str):

    logging.info("Performing Exploratory Data Analysis...")
    sns.set_style('whitegrid')

    # 1. Histograms
    original_df[['age', 'trestbps', 'chol', 'thalach']].hist(figsize=(12, 10), bins=20)
    plt.suptitle("Histograms of Key Numerical Features")
    plt.savefig(f"{prefix}_histograms.png")
    plt.show()

    # 2. Correlation Heatmap
    plt.figure(figsize=(18, 15))
    sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')
    plt.title("Feature Correlation Heatmap")
    plt.savefig(f"{prefix}_correlation_heatmap.png")
    plt.show()

    # 3. Boxplot
    plt.figure(figsize=(8, 6))
    sns.boxplot(x='target', y='chol', data=original_df)
    plt.title("Cholesterol Levels by Heart Disease Presence")
    plt.xlabel("Heart Disease (0 = No, >0 = Yes)")
    plt.ylabel("Cholesterol")
    plt.savefig(f"{prefix}_cholesterol_boxplot.png")
    plt.show()

def main():

    logging.info("Starting the data preprocessing pipeline...")

    # Execute the pipeline steps
    original_df = load_data(CONFIG["data_url"], CONFIG["column_names"])
    df_clean = handle_missing_values(original_df.copy(), CONFIG["cols_to_impute"])
    df_encoded = encode_categorical_features(df_clean, CONFIG["categorical_cols"])
    df_processed = scale_numerical_features(df_encoded, CONFIG["numerical_cols_to_scale"])

    logging.info("Final processed DataFrame head:")
    print(df_processed.head())

    # Perform EDA on the data
    perform_exploratory_data_analysis(
        df=df_processed,
        original_df=original_df,
        prefix=CONFIG["output_path_prefix"]
    )

    # Perform feature selection
    logging.info("Columns in df_processed before feature selection:")
    print(df_processed.columns)
    selected_features = select_features(df_processed, 'target')

    # Save the selected features
    joblib.dump(selected_features, 'selected_features.joblib')
    logging.info("Selected features saved to 'selected_features.joblib'")

    # Tune hyperparameters and get the best model
    best_model = tune_model_hyperparameters(df_processed[selected_features], df_processed['target'])

    # Export the best model
    export_model(best_model, 'final_model.pkl')
    logging.info("Best model saved to 'final_model.pkl'")

    # Perform clustering (optional)
    perform_clustering(df_processed.drop(columns=['target']))


    logging.info("Pipeline finished successfully.")


if __name__ == "__main__":
    main()

from sklearn.decomposition import PCA
from typing import Tuple

def apply_pca(df: pd.DataFrame, target_col: str, n_components: int = None) -> Tuple[pd.DataFrame, PCA]:

    logging.info("Applying PCA for dimensionality reduction...")
    X = df.drop(columns=[target_col])
    y = df[target_col]

    pca = PCA(n_components=n_components)
    X_pca = pca.fit_transform(X)

    # Plotting the cumulative explained variance
    plt.figure(figsize=(10, 7))
    plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle='--')
    plt.title("Explained Variance by Principal Components")
    plt.xlabel("Number of Components")
    plt.ylabel("Cumulative Explained Variance")
    plt.grid(True)
    plt.show()
    logging.info(f"PCA applied. Explained variance with {pca.n_components_} components: {sum(pca.explained_variance_ratio_):.2f}")

    pca_df = pd.DataFrame(X_pca, columns=[f'PC_{i+1}' for i in range(X_pca.shape[1])])
    return pca_df, pca


from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFE

def select_features(df: pd.DataFrame, target_col: str, n_features_to_select: int = 10) -> List[str]:

    logging.info("Performing feature selection...")
    # Separate features (X) and target (y)
    X = df.drop(columns=[target_col])
    y = df[target_col]

    # 1. Feature Importance from Random Forest
    model_rf = RandomForestClassifier(n_estimators=100, random_state=42)
    model_rf.fit(X, y)

    importances = pd.Series(model_rf.feature_importances_, index=X.columns).sort_values(ascending=False)

    plt.figure(figsize=(12, 8))
    sns.barplot(x=importances, y=importances.index)
    plt.title("Feature Importance from Random Forest")
    plt.show()
    logging.info(f"Top 5 features from RF: {importances.head(5).index.tolist()}")

    # 2. Recursive Feature Elimination (RFE)
    rfe = RFE(estimator=model_rf, n_features_to_select=n_features_to_select)
    rfe.fit(X, y)

    selected_features = X.columns[rfe.support_].tolist()
    logging.info(f"Features selected by RFE: {selected_features}")

    return selected_features

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc

def train_and_evaluate_classifiers(X: pd.DataFrame, y: pd.Series) -> Dict:

    logging.info("Training and evaluating classification models...")
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

    models = {
        "Logistic Regression": LogisticRegression(max_iter=1000),
        "Decision Tree": DecisionTreeClassifier(random_state=42),
        "Random Forest": RandomForestClassifier(random_state=42),
        "SVM": SVC(probability=True, random_state=42)
    }

    results = {}

    plt.figure(figsize=(10, 8))
    for name, model in models.items():
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        y_prob = model.predict_proba(X_test)[:, 1]

        metrics = {
            "accuracy": accuracy_score(y_test, y_pred),
            "precision": precision_score(y_test, y_pred, average='weighted'),
            "recall": recall_score(y_test, y_pred, average='weighted'),
            "f1_score": f1_score(y_test, y_pred, average='weighted')
        }
        results[name] = {"model": model, "metrics": metrics}
        logging.info(f"--- {name} ---")
        logging.info(f"Metrics: {metrics}")

        fpr, tpr, _ = roc_curve(y_test, y_prob, pos_label=1)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')

    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend(loc='lower right')
    plt.grid()
    plt.show()

    return results


from sklearn.cluster import KMeans, AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage

def perform_clustering(X: pd.DataFrame):

    logging.info("Performing unsupervised clustering...")

    # 1. K-Means with Elbow Method
    inertia = []
    k_range = range(1, 11)
    for k in k_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(X)
        inertia.append(kmeans.inertia_)

    plt.figure(figsize=(10, 6))
    plt.plot(k_range, inertia, marker='o')
    plt.title('Elbow Method for Optimal K')
    plt.xlabel('Number of Clusters (K)')
    plt.ylabel('Inertia')
    plt.grid()
    plt.show()

    # 2. Hierarchical Clustering Dendrogram
    linked = linkage(X, method='ward')

    plt.figure(figsize=(15, 8))
    dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)
    plt.title('Hierarchical Clustering Dendrogram')
    plt.xlabel('Sample Index')
    plt.ylabel('Distance')
    plt.show()


from sklearn.model_selection import GridSearchCV

def tune_model_hyperparameters(X: pd.DataFrame, y: pd.Series):

    logging.info("Tuning hyperparameters for Random Forest...")
    X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

    param_grid = {
        'n_estimators': [100, 200, 300],
        'max_depth': [None, 10, 20, 30],
        'min_samples_leaf': [1, 2, 4]
    }

    rf = RandomForestClassifier(random_state=42)
    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)
    grid_search.fit(X_train, y_train)

    logging.info(f"Best parameters found: {grid_search.best_params_}")
    best_model = grid_search.best_estimator_
    return best_model


import joblib

def export_model(model, filename: str):
    logging.info(f"Exporting model to {filename}...")
    joblib.dump(model, filename)
    logging.info("Model exported successfully.")

# Save this code as app.py
import streamlit as st
import pandas as pd
import joblib

# Load the trained model and feature list
# Make sure 'final_model.pkl' and 'selected_features.joblib' are in the same folder
try:
    model = joblib.load('final_model.pkl')
    features = joblib.load('selected_features.joblib') # You need to save this list too!
except FileNotFoundError:
    st.error("Model or feature list not found. Please train and export them first.")
    st.stop()

st.title("Heart Disease Prediction App")
st.write("Enter patient data to predict the likelihood of heart disease.")

# Create input fields for all selected features
input_data = {}
for feature in features:
    # Use number_input for all features for simplicity in this example
    input_data[feature] = st.number_input(f"Enter {feature}", value=0.0)

if st.button("Predict"):
    # Create a DataFrame from the input
    input_df = pd.DataFrame([input_data])

    # Ensure column order is the same as during training
    input_df = input_df[features]

    # Make prediction
    prediction_proba = model.predict_proba(input_df)[0][1] # Probability of disease

    st.subheader("Prediction Result")
    st.write(f"The predicted probability of heart disease is: **{prediction_proba:.2f}**")

    if prediction_proba > 0.5:
        st.warning("The model predicts a high risk of heart disease.")
    else:
        st.success("The model predicts a low risk of heart disease.")
